kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: lvm-volume-provisioner
provisioner: pingcap.com/lvm-volume-provisioner
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: lvm-scheduler
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: lvm-volume-manager
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: lvm-volume-provisioner
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: default:lvm-scheduler
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["endpoints", "persistentvolumeclaims"]
  verbs: ["get", "list", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: default:lvm-volume-provisioner
rules:
- apiGroups: [""]
  resources: ["events"]
  verbs: ["get", "list", "create", "watch", "patch"]
- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["get", "list", "watch", "update"]
- apiGroups: [""]
  resources: ["persistentvolumes", "persistentvolumeclaims"]
  verbs: ["*"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: default:lvm-volume-provisioner
subjects:
- kind: ServiceAccount
  name: lvm-volume-provisioner
  namespace: default
roleRef:
  kind: ClusterRole
  name: default:lvm-volume-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: default:lvm-scheduler
subjects:
- kind: ServiceAccount
  name: lvm-scheduler
  namespace: default
roleRef:
  kind: ClusterRole
  name: default:lvm-scheduler
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: default:kube-scheduler
subjects:
- kind: ServiceAccount
  name: lvm-scheduler
  namespace: default
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: default:kube-scheduler
subjects:
- kind: ServiceAccount
  name: lvm-scheduler
  namespace: default
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: default:lvm-volume-manager
rules:
- apiGroups: [""]
  resources: ["persistentvolumes", "persistentvolumeclaims"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["pods", "nodes"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["nodes/status"]
  verbs: ["patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: default:lvm-volume-manager
subjects:
- kind: ServiceAccount
  name: lvm-volume-manager
  namespace: default
roleRef:
  kind: ClusterRole
  name: default:lvm-volume-manager
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: lvm-scheduler-policy
data:
  policy.cfg: |-
    {
            "kind" : "Policy",
            "apiVersion" : "v1",
            "predicates": [
                    {"name": "MatchNodeSelector"},
                    {"name": "PodFitsResources"},
                    {"name": "PodFitsHostPorts"},
                    {"name": "HostName"},
                    {"name": "NoDiskConflict"},
                    {"name": "PodToleratesNodeTaints"},
                    {"name": "CheckNodeMemoryPressure"},
                    {"name": "CheckNodeDiskPressure"},
                    {"name": "MatchInterPodAffinity"},
                    {"name": "GeneralPredicates"}
            ],
            "priorities": [
                    {"name": "EqualPriority", "weight": 1},
                    {"name": "ImageLocalityPriority", "weight": 1},
                    {"name": "LeastRequestedPriority", "weight": 1},
                    {"name": "BalancedResourceAllocation", "weight": 1},
                    {"name": "SelectorSpreadPriority", "weight": 1},
                    {"name": "NodePreferAvoidPodsPriority", "weight": 1},
                    {"name": "NodeAffinityPriority", "weight": 1},
                    {"name": "TaintTolerationPriority", "weight": 1},
                    {"name": "MostRequestedPriority", "weight": 1}
            ],
            "extenders": [
                    {
                            "urlPrefix": "http://127.0.0.1:10262/scheduler",
                            "filterVerb": "filter",
                            "weight": 1,
                            "httpTimeout": 30000000000,
                            "enableHttps": false
                    }
            ]
    }

---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: lvm-volume-manager
spec:
  selector:
    matchLabels:
      app: lvm-volume-manager
  template:
    metadata:
      labels:
        app: lvm-volume-manager
    spec:
      serviceAccount: lvm-volume-manager
      containers:
      - name: lvm-volume-manager
        image: localhost:5000/pingcap/lvm-manager:latest
        securityContext:
          privileged: true
        command:
        - lvm-volume-manager
        - --fs-type=ext4
        - --workers=5
        - --base-dir=/data
        - --domain-name=pingcap.com
        - --logtostderr
        volumeMounts:
        - name: data
          mountPath: /data
        env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
      volumes:
      - name: data
        hostPath:
          path: /data
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: lvm-provisioner
  labels:
    app: lvm-provisioner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: lvm-provisioner
  template:
    metadata:
      labels:
        app: lvm-provisioner
    spec:
      serviceAccount: lvm-volume-provisioner
      containers:
      - name: lvm-provisioner
        image: localhost:5000/pingcap/lvm-manager:latest
        command:
        - lvm-volume-provisioner
        - --domain-name=pingcap.com
        - --kube-version=v1.9.5
        - --logtostderr
        volumeMounts:
        - name: timezone
          mountPath: /etc/localtime
      volumes:
      - name: timezone
        hostPath:
          path: /etc/localtime
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: lvm-scheduler
  labels:
    app: lvm-scheduler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: lvm-scheduler
  template:
    metadata:
      labels:
        app: lvm-scheduler
    spec:
      serviceAccount: lvm-scheduler
      containers:
      - name: lvm-scheduler
        image: localhost:5000/pingcap/lvm-manager:latest
        command:
          - lvm-scheduler
          - --port=10262
          - --logtostderr
        env:
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        volumeMounts:
          - name: timezone
            mountPath: /etc/localtime
      - name: kube-scheduler
        image: uhub.ucloud.cn/pingcap/hyperkube:v1.9.5
        command:
        - /hyperkube
        - scheduler
        - --port=10261
        - --leader-elect=true
        - --lock-object-name=lvm-scheduler
        - --lock-object-namespace=default
        - --scheduler-name=lvm-scheduler
        - --v=4
        - --policy-configmap=lvm-scheduler-policy
        - --policy-configmap-namespace=default
        # TODO: find the reason why health-check failed and uncomment following lines
        # livenessProbe:
        #   httpGet:
        #     host: 127.0.0.1
        #     path: /healthz
        #     port: 10261
        #   initialDelaySeconds: 30
        #   timeoutSeconds: 10
        volumeMounts:
          - name: timezone
            mountPath: /etc/localtime
      volumes:
      - name: timezone
        hostPath:
          path: /etc/localtime
